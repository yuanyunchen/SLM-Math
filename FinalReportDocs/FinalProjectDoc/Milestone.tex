 \documentclass{article}

\usepackage[final]{neurips_2019}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{float}

\title{
  SLM-Math: Empowering Small Language Models for Mathematical Reasoning \\
  \vspace{1em}
  \small{\normalfont Columbia COMS4705 Project Milestone} \\
  \small{\normalfont \textbf{Keywords:} \textit{Small Language Models, Reinforcement Learning, Mathematical Reasoning, Multi-Agent Systems, Chain-of-Thought}}
}

\author{
  Roger Wang \\
  Department of Computer Science \\
  Columbia University \\
  \texttt{lw3240@columbia.edu} \\
  \And
  Jinzi Luo \\
  Department of Computer Science \\
  Columbia University \\
  \texttt{jl7199@columbia.edu} \\
  \And
  Yunchen Yuan \\
  Department of Computer Science \\
  Columbia University \\
  \texttt{yy3610@columbia.edu}
}

\begin{document}

\maketitle


\begin{abstract}
We investigate enhancing small language models for mathematical reasoning through a four-stage pipeline: (1) CoT data generation via Grok-4.1 and MiniMax-M2, (2) supervised fine-tuning (full and LoRA) on Qwen2.5-Math-1.5B, (3) GRPO reinforcement learning, and (4) ten agentic workflow architectures. Results demonstrate substantial gains from both training-based improvements and inference-time multi-agent verification, with cross-dataset transfer from GSM8K to MATH and consistent benefits from Solver-Checker architectures.
\end{abstract}


\section{Key Information}
\begin{itemize}
    \item TA mentor: Melody Ma
    \item External collaborators: No
    \item Sharing project: No
\end{itemize}


\section{Approach}

\subsection{Baseline Model}
We employ Qwen2.5-Math-1.5B~\cite{yang2024qwen2} as our base model, a small language model with 1.5 billion parameters that has been pretrained on extensive mathematical corpora including textbooks, problem sets, and solution traces. This domain-specific pretraining provides strong mathematical foundations while maintaining computational efficiency suitable for our multi-stage training pipeline.

\subsection{Implementation Pipeline}
We implement a four-stage pipeline combining training-based improvements with inference-time agentic workflows.

\subsubsection{Data Pipeline}
We generate high-quality CoT data using a two-round cascade: Grok-4.1-Reasoning-Fast followed by MiniMax-M2 with multiple attempts. Each attempt applies configurable thinking effort (300--2500 tokens) with structured output format: \texttt{<think>...</think>} reasoning followed by solution steps ending in \texttt{\textbackslash boxed\{answer\}}. Only solutions matching ground truth answers are retained. From 19,473 training samples (7,473 GSM8K~\cite{cobbe2021training} + 12,000 MATH~\cite{hendrycks2021measuring}), we obtain 18,946 verified CoT samples, filtering 527 unsolved problems (2.71\%).


\subsubsection{Supervised Fine-Tuning}
We train Qwen2.5-Math-1.5B using GSM8K CoT data in two configurations: (1) \textbf{Full SFT}: LR \(5 \times 10^{-5}\) (2) \textbf{LoRA (r=16)}~\cite{hu2021lora}: LR \(1 \times 10^{-4}\). Both use cosine scheduling and bfloat16, trained on batch size 128 with 2 epochs . Training exhibits rapid convergence and demonstrates transfer from GSM8K to MATH-500, suggesting general reasoning improvement rather than memorization.

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.7\textwidth]{figures/sft_loss_curve.pdf}
% \caption{Training loss curves for LoRA and Full SFT showing rapid convergence.}
% \end{figure}

\subsubsection{Reinforcement Learning}
We apply GRPO~\cite{shao2024deepseekmath} to the SFT LoRA checkpoint with binary rewards (\(r=\pm 1.0\)) and KL regularization (\(\beta=0.05\)): \(\mathcal{L}_{\text{GRPO}} = -\mathbb{E}[r \cdot \log \pi_\theta] + \beta \, \text{KL}(\pi_\theta \| \pi_{\text{ref}})\). Training uses \(K=2\) responses/prompt, batch 16, LR \(5 \times 10^{-6}\), 1 epochs on GSM8K. Policy optimization provides further gains beyond SFT by directly rewarding correct answers.

\subsubsection{Agentic Workflows}
% We implement nine inference-time agent architectures without additional training, inspired by multi-agent reasoning systems~\cite{wu2023mathchat,Guanetal2025}: (1) \textbf{Solver-Checker} variants (Base, Chat, Stateless, Trivial, With-Tools, Summarizer) enable iterative verification over 5 rounds; (2) \textbf{Majority Vote} aggregates multiple solutions; (3) \textbf{Plan-and-Reflection} decomposes problems into sub-tasks; (4) \textbf{Python Tools} executes code for precise computation. These workflows improve accuracy through structured reasoning, with tool-augmented agents addressing arithmetic errors effectively.

% We implement nine training-free inference-time agent architectures~\cite{wu2023mathchat,Guanetal2025}:
% (1) \textbf{Solver-Checker} variants (Base, Stateless, Chat, Trivial-Chat, Summarizer, With-Tools) use iterative verification loops where a solver generates solutions and a checker validates them, with variants differing in conversation management (stateless vs. chat-based), summarization layers to reduce noise, and tool execution for code-based verification.
% (2) \textbf{Majority Vote} aggregates multiple inference runs with different random seeds via voting.
% (3)  \textbf{Plan-and-Reflection} uses a 3-phase solve-verify-tiebreaker workflow.
% (4)  \textbf{Python Tools}  executes generated code blocks for precise computation.
% All agents share deterministic first-round decoding for fair accuracy comparison.
We implement nine training-free inference-time agent architectures~\cite{wu2023mathchat,Guanetal2025}:
(1) \textbf{Solver-Checker} variants (Stateless, Chat, Summarizer, With-Tools) use iterative verification loops where a solver generates solutions and a checker validates them, with variants differing in conversation management (stateless uses independent prompts; chat-based maintains dialogue history), optional summarization layers to reduce context noise, and tool execution for code-based numerical verification.
(2) \textbf{Majority Vote} aggregates multiple inference runs---first deterministic, then with different random seeds---via voting with first-run preference.
(3) \textbf{Python Tools} is a single-shot agent that executes generated code blocks for precise computation without iterative verification.
All agents share deterministic first-round decoding for fair accuracy comparison.


\section{Experiments}

\textbf{Datasets:}  We use 500 samples from GSM8K test set and Math-500 dataset for evaluation. \textbf{Metrics:} Pass@1 accuracy.

\subsection*{Results}

\begin{table}[H]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Configuration}  & \textbf{GSM8K-test} & \textbf{MATH-500} \\
\midrule
\multicolumn{3}{l}{\textit{Base Model}} \\
Qwen2.5-Math-1.5B & 65.8\% & 53.2\% \\
\midrule
\multicolumn{3}{l}{\textit{Training-based Methods}} \\
SFT (LoRA r=16) & 80.0\% & \textbf{67.2\%} \\
SFT (Full) & \textbf{81.6\%}  & 67.0\% \\
\midrule
\multicolumn{3}{l}{\textit{Agentic Workflows}} \\
Solver-Checker With-Tools & 81.4\%  & 49.8\% \\
Majority Vote & 70.2\% & 54.8\% \\
Agent with Python Tools & 72.6\%  & 45.2\% \\
\bottomrule
\end{tabular}
\caption{Pass@1 accuracy across training methods and agent architectures. GSM8K-test uses 500 samples.}
\label{tab:results}
\end{table}



\subsection*{Analysis}
Our results demonstrate substantial improvements through both training and inference-time interventions. SFT yields rapid convergence with both LoRA and full fine-tuning producing comparable results. Cross-dataset transfer from GSM8K to MATH-500 indicates general reasoning improvement. GRPO training is ongoing due to its higher computational demandsâ€”generating multiple candidate responses per prompt and computing reward signals. Agentic workflows, particularly Solver-Checker with Tools, Majority Vote, and Agent with Python Tools outperform the baseline by leveraging external computation delegation and ensemble robustness. Other tested workflows (summarizer variants, stateless checkers, summarizer) exhibited performance degradation (gsm8k-test 46.8\% to 59.4\%.) due to: insufficient model parameters for complex multi-agent coordination, limited context windows constraining elaborate prompts, and over-specialized role designs reducing cross-task generalization.

% Tool-augmented approaches bypass arithmetic unreliability; majority voting reduces variance through answer aggregation. 

% Agentic workflows, particularly Solver-Checker and tool-augmented variants, offer consistent gains through iterative verification and error correction. However, small models exhibit capacity constraints with complex multi-agent coordination beyond two-agent systems, suggesting the need to calibrate agent complexity to model size. Tool-augmented agents effectively address arithmetic unreliability by delegating computation to external verifiers.



\section{Future Work}

Our future directions include: (1) \textbf{Training Optimization and Data Expansion}: optimizing training configurations and hyperparameters, expanding to full MATH dataset and competition problems (AIME, AMC)~\cite{Jia2024AIME2024}; (2) \textbf{Advanced RL Methods}: improving reward mechanisms through fine-grained rule design (formatting, step validity), reward models, and process supervision; exploring advanced sampling strategies integrating Monte Carlo Tree Search (MCTS)~\cite{Guanetal2025} for more effective exploration; (3) \textbf{Agentic RL} (most important): training models through agentic workflows combined with domain expert models~\cite{Park2025MAPoRLMPA}, enabling joint optimization of multi-agent reasoning and policy learning to achieve synergistic improvements beyond independent training and inference-time augmentation.

\section{Response to TA Feedback}

\textbf{Q1: How do we blend SFT and multi-agent RL?} We first apply SFT to establish stable reasoning behavior using high-quality CoT traces, providing a warm initialization. Subsequently, we introduce GRPO reinforcement learning to optimize the policy through iterative sampling and outcome-based rewards, building upon the SFT foundation while enabling exploration of diverse solution strategies.

\textbf{Q2: Where do teacher traces come from?} Our teacher traces are generated by frontier reasoning models (Grok-4.1-Reasoning-Fast and MiniMax-M2) through our two-round data pipeline. These external expert models produce step-by-step solutions with explicit reasoning processes, which are verified against ground truth answers before being used as training data for the student model.

\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
