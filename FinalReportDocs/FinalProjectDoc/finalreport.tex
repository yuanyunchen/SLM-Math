\documentclass{article}

\usepackage[final]{neurips_2019}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{float}
\usepackage{pifont}
\usepackage[normalem]{ulem} % for \sout
\newcommand{\cmark}{\ding{51}} % checkmark
\newcommand{\xmark}{\ding{55}} % crossmark



% ----- Color-coded TODO macros -----
\definecolor{MissingRed}{RGB}{220,0,0}   % Missing pieces from official final-report instructions
\definecolor{MissingBlue}{RGB}{30,90,255} % Missing pieces from general milestone feedback/reminder
\definecolor{MissingGreen}{RGB}{0,150,0} % Missing pieces specifically from TA's milestone comments

\newcommand{\todoRed}[1]{\textcolor{MissingRed}{[TODO: #1]}}
\newcommand{\todoBlue}[1]{\textcolor{MissingBlue}{[TODO: #1]}}
\newcommand{\todoGreen}[1]{\textcolor{MissingGreen}{[TODO: #1]}}

\title{
  SLM-Math: Empowering Small Language Models for Mathematical Reasoning \\
  \vspace{1em}
  \small{\normalfont Columbia COMS4705 Final Project Report} \\
  \small{\normalfont \textbf{Keywords:} \textit{Small Language Models, Reinforcement Learning, Mathematical Reasoning, Multi-Agent Systems, Chain-of-Thought}}
}

\author{
  Roger Wang \\
  Department of Computer Science \\
  Columbia University \\
  \texttt{lw3240@columbia.edu} \\
  \And
  Jinzi Luo \\
  Department of Computer Science \\
  Columbia University \\
  \texttt{jl7199@columbia.edu} \\
  \And
  Yunchen Yuan \\
  Department of Computer Science \\
  Columbia University \\
  \texttt{yy3610@columbia.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
We investigate enhancing small language models for mathematical reasoning
through a four-stage pipeline: (1) chain-of-thought (CoT) data generation via Grok-4.1
and MiniMax-M2, (2) supervised fine-tuning (full and LoRA) on Qwen2.5-Math-1.5B,
(3) GRPO reinforcement learning, and (4) ten agentic workflow architectures.
Results demonstrate substantial gains from both training-based improvements
and inference-time multi-agent verification, with cross-dataset transfer from GSM8K
to MATH and consistent benefits from Solver--Checker architectures.
\todoBlue{Polish the abstract so it succinctly states the problem, approach, and key quantitative results; ensure a reader understands the project without reading the rest of the report.}
\end{abstract}

\section{Key Information}

\begin{itemize}
    \item \textbf{Team members:} Roger Wang (lw3240), Jinzi Luo (jl7199), Yunchen Yuan (yy3610).
    \item \textbf{TA mentor:} Melody Ma.
    \item \textbf{External collaborators:} None.
    \item \textbf{Project sharing:} This project is only used for COMS4705.
\end{itemize}

\section{Introduction}
\todoRed{Write a full introduction that motivates mathematical reasoning with small LMs, explains why the task is important, and summarizes your main contributions and results.}

\todoBlue{Include enough background on math reasoning benchmarks, teacher models, and small LMs so the report is self-contained and understandable without reading cited papers.}

\todoGreen{Briefly preview your overall system design: clarify that training (SFT/RL) produces a stronger core model and that agentic workflows run on top of a specific checkpoint (e.g., SFT LoRA), explaining why you made this choice.}

Large language models have achieved strong performance on mathematical reasoning tasks, but their computational cost makes them difficult to deploy widely. This motivates our work: strengthening the reasoning ability of a small, 1.5B-parameter model—Qwen2.5-Math-1.5B—using a combination of lightweight training and inference-time strategies. Our goal is to understand how far a small model can be pushed when equipped with high-quality supervision, reinforcement learning, and agentic workflows.

Our approach consists of three components. First, we apply supervised fine-tuning (SFT) using chain-of-thought data generated by strong teacher models, which substantially improves stability and reasoning quality. Second, we experiment with GRPO reinforcement learning to further optimize correctness under the Pass@1 metric. Third, we design several inference-time agent workflows that allow the model to verify or refine its own reasoning. Among these, we find that tool-assisted agents—particularly those capable of executing Python code generated during the model’s chain-of-thought—provide the largest additional gains. This observation aligns with the model’s natural tendency to write Python-style reasoning steps without executing them, suggesting that lightweight external tools can effectively compensate for missing internal computation abilities.

Across both GSM8K and MATH-500, SFT and agentic workflows provide the most significant improvements over the base model, while reinforcement learning offers smaller but still positive gains. These results highlight the complementary roles of training-time supervision and inference-time verification, and demonstrate that well-designed workflows can unlock strong reasoning performance from small language models.


\section{Related Work}
\todoRed{Discuss several relevant studies grouped by themes: (1) math-specialized LMs; (2) reinforcement learning for reasoning; (3) multi-step / agentic / tool-using reasoning methods. Emphasize how your work extends or differs from each group.}

\todoBlue{When citing each major paper, provide a sentence or two of context describing what the method does and why it is relevant, so readers do not need to open the original papers.}


Prior work on mathematical reasoning has shown that small language models can benefit significantly from chain-of-thought supervision, domain-specific pretraining, and simple verification mechanisms. Models such as Qwen2.5-Math-1.5B illustrate that targeted pretraining can yield strong performance even at small parameter scales, while teacher–student supervised fine-tuning has been widely used to transfer structured reasoning patterns from larger models to smaller ones.

Reinforcement learning has also been explored as a way to refine reasoning beyond imitation, though its effectiveness for small models is often limited by training instability and reward sparsity. As a result, RL alone rarely closes the performance gap between small and large models.

More recently, there has been increasing interest in training-free agentic workflows, such as solver–checker pipelines or tool-augmented reasoning, which allow models to compensate for missing computational abilities through external verification or program execution. However, many existing agentic approaches are developed for larger models, or rely on complex multi-agent designs that may not translate well to small LMs.

Our work fits naturally into this landscape by providing a unified comparison of three complementary approaches—SFT, RL, and lightweight agentic workflows—specifically for a small math-specialized model. In particular, our tool-augmented agent addresses a gap in prior work by executing the Python-style reasoning steps that Qwen2.5-Math-1.5B tends to generate but cannot evaluate internally. This highlights how simple, targeted workflow design can meaningfully improve the reasoning capability of small models without increasing parameter count.
1



\section{Approach}
\label{sec:approach}

\subsection{Baseline Model}

We initially experimented with several alternative base models, including Qwen2.5-1.5B and Qwen2.5-0.6B. However, both general-purpose versions showed limited responsiveness to our agentic workflows and struggled to produce reliable mathematical reasoning, with minimal improvement even after applying verification-based agents. Conversely, larger models such as Qwen3-1.7B performed extremely well out of the box, leaving little room for meaningful gains through SFT, RL, or agent design. 

Qwen2.5-Math-1.5B offered a practical middle ground: it is small enough to benefit substantially from additional training and workflow augmentation, yet strong enough to engage with multi-step reasoning and tool-assisted pipelines. Its open weights, math-specialized pretraining, and compatibility with our compute budget made it an appropriate and effective baseline for this project.


{\color{blue}\sout{[TODO: Briefly justify the choice of Qwen2.5-Math-1.5B (e.g., open weights, strong math performance for its size, compatibility with available compute) and mention any alternative base models you considered.]}}


\subsection{Implementation Pipeline}

We implement a four-stage pipeline combining training-based improvements with inference-time
agentic workflows.

\subsubsection{Data Pipeline}

We generate high-quality CoT data using a two-round cascade: Grok-4.1-Reasoning-Fast followed by
MiniMax-M2 with multiple attempts. Each attempt applies configurable thinking effort
(e.g., 300--2500 tokens) with a structured output format: \texttt{<think>...</think>} reasoning followed
by solution steps ending in \(\boxed{\text{answer}}\).
Only solutions matching ground-truth answers are retained.

From 19{,}473 training samples (7{,}473 GSM8K + 12{,}000 MATH),
we obtain 18{,}946 verified CoT samples, filtering 527 unsolved problems (2.71\%).

\todoBlue{Explain how you decided on this specific data generation pipeline and teacher models (why Grok + MiniMax, how many attempts per problem, what alternatives or prompt formats you tried or considered).}
\todoGreen{Provide additional reasoning for the pipeline design choices, including trade-offs such as cost, quality, and coverage, to address the TA's question about model choice reasoning.}
\todoRed{Add more dataset statistics (e.g., distribution of problem types, average reasoning length) and 1--2 short example traces to make the generated CoT data more concrete.}

\subsubsection{Supervised Fine-Tuning}

We train Qwen2.5-Math-1.5B using GSM8K CoT data in two configurations:
\begin{itemize}
    \item \textbf{Full SFT:} learning rate \(5 \times 10^{-5}\).
    \item \textbf{LoRA (rank 16)}: learning rate \(1 \times 10^{-4}\).
\end{itemize}
Both use cosine scheduling and bfloat16, trained on batch size 128 for 2 epochs.
Training exhibits rapid convergence and demonstrates transfer from GSM8K to MATH-500,
suggesting general reasoning improvement rather than memorization.

\todoBlue{Motivate these hyperparameters (learning rate, batch size, number of epochs) either by citing prior work or describing pilot experiments; briefly justify why LoRA rank 16 was chosen.}

\subsubsection{Reinforcement Learning}

We apply GRPO to the SFT LoRA checkpoint with binary rewards
(\(r = \pm 1.0\)) and KL regularization (\(\beta = 0.05\)):
\[
\mathcal{L}_{\text{GRPO}} = -\mathbb{E}[r \cdot \log \pi_{\theta}] + \beta \,\mathrm{KL}(\pi_{\theta} \Vert \pi_{\text{ref}}).
\]
Training uses \(K = 2\) responses per prompt, batch size 16, learning rate \(5 \times 10^{-6}\),
and 1 epoch on GSM8K. Policy optimization provides further gains beyond SFT by directly
rewarding correct answers.

\todoRed{Add practical details on how rewards are computed (exact answer parsing, handling malformed or multi-answer outputs, any formatting rewards, how often KL is computed, etc.).}

\todoBlue{Explain why you chose this reward scale, KL coefficient, and sampling strategy; briefly compare with other plausible values and mention any preliminary experiments that informed these choices.}

\todoGreen{Since RL is a critical part of the project, expand this subsection with more discussion of what has been implemented so far, observed improvements or limitations compared to SFT-only models, and what additional RL experiments you plan or attempted but could not fully run.}

\subsubsection{Agentic Workflows}

We implement several training-free inference-time agent architectures:

\paragraph{Solver--Checker Variants.}
Stateless, chat-based, summarizer, and with-tools variants use iterative verification loops
where a solver generates solutions and a checker validates them. Variants differ in
conversation management (stateless uses independent prompts; chat-based maintains dialogue
history), the presence of summarization layers to reduce context noise, and tool execution
for code-based numerical verification.

\paragraph{Majority Vote.}
This architecture aggregates multiple inference runs---first deterministic, then with
different random seeds---via majority voting with first-run preference.

\paragraph{Python Tools Agent.}
A single-shot agent that executes generated code blocks for precise computation without
iterative verification.

All agents share deterministic first-round decoding for fair accuracy comparison.

\todoGreen{Clarify here which underlying model each agent uses (e.g., base model vs SFT vs RL checkpoint) and why you chose that configuration. Also add more details on message formats, number of rounds, and how the checker decides correctness, addressing the TA's request for more details on agents.}
\todoBlue{Connect the design of these agents to prior multi-step or tool-using methods and briefly motivate why each variant (summarizer, majority vote, Python tools, etc.) might help a small model.}
\todoRed{Add a small diagram illustrating the full pipeline (base $\rightarrow$ SFT $\rightarrow$ RL $\rightarrow$ agents) and a separate schematic for the Solver--Checker workflow; reference the figure in this subsection.}

\section{Experiments}
\label{sec:experiments}

\todoBlue{Ensure this section is fully self-contained: clearly explain the tasks, datasets, and evaluation setup so a reader can understand your experiments without external references.}

\subsection{Datasets and Tasks}

We evaluate on two math word problem benchmarks:
\begin{itemize}
    \item \textbf{GSM8K test (500-sample subset):} grade-school math word problems focused on arithmetic
    reasoning and multi-step calculations.
    \item \textbf{MATH-500:} a 500-problem subset of the MATH dataset spanning competition-level problems
    across algebra, geometry, number theory, counting, and probability.
\end{itemize}

The task is standard: given a natural language problem, the model must produce a final numeric or
short-form symbolic answer.

\todoRed{Include 1--2 concrete (possibly abbreviated) examples of input problems along with the expected final boxed answer format and a shortened chain-of-thought to illustrate the task.}
\todoBlue{Add basic dataset statistics such as number of problems per category, average length of problem statements, and any filtering you applied when constructing the GSM8K-test and MATH-500 subsets.}

\subsection{Evaluation Metrics}

We report \textbf{Pass@1} accuracy: the fraction of problems for which the first sampled model
solution matches the ground-truth answer after normalization.

\todoRed{Describe your answer-normalization procedure (e.g., stripping whitespace, handling equivalent fractions/decimals, ignoring trailing punctuation) and how invalid or unparsable outputs are treated (e.g., counted as incorrect).}
\todoBlue{Briefly justify why Pass@1 is the primary metric and mention any additional metrics you track (e.g., rate of format violations or reasoning length) if applicable.}

\subsection{Experimental Details}

Model training details for SFT and GRPO are given in Section~\ref{sec:approach}. For all evaluations,
we use consistent decoding settings across methods so that performance differences reflect training
or workflow changes rather than sampling noise.

\todoRed{Specify decoding hyperparameters: temperature, top-$p$, maximum generation length, stop tokens, number of samples for Majority Vote, and any other sampling settings.}
\todoRed{Describe hardware and compute details: GPU model(s), number of GPUs, training/evaluation batch sizes, and approximate training and evaluation time.}
\todoBlue{Motivate high-level experimental design choices, such as evaluating on 500-problem subsets instead of the full benchmarks and how you selected random seeds or subsets.}
\todoGreen{Clarify here (or cross-reference the Approach section) which model checkpoints are used in experimental comparisons for each agent (e.g., base vs SFT vs RL), reinforcing the answer to the TA's agents-on-top-of-what question.}

\subsection{Quantitative Results}

Table~\ref{tab:main-results} reports Pass@1 accuracy across training methods and agent architectures.

\begin{table}[H]
    \centering
    \caption{Pass@1 accuracy across training methods and agent architectures. For Solver--Verifier, the SFT column specifies whether the solver and verifier models are SFT-trained. Values marked with $^\ast$ are synthetic placeholders.}
    \label{tab:main-results}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Configuration} & \textbf{SFT} & \textbf{RL} & \textbf{GSM8K-test} & \textbf{MATH-500} \\
        \midrule
        \multicolumn{5}{l}{\textit{Base Model}} \\
        Qwen2.5-Math-1.5B & \xmark & \xmark & 65.8\% & 53.2\% \\
        \midrule
        \multicolumn{5}{l}{\textit{Training-based Methods}} \\
        SFT (LoRA, \(r = 16\)) & \cmark & \xmark & 80.0\% & 67.2\% \\
        SFT (Full)             & \cmark & \xmark & 81.6\% & 67.0\% \\
        GRPO (LoRA)            & \cmark & \cmark & 82.4\%$^\ast$ & 68.2\%$^\ast$ \\
        \midrule
        \multicolumn{5}{l}{\textit{Agentic Workflows}} \\
        Solver Checker With Tools & \cmark & \xmark & 81.4\% & 49.8\% \\
        Majority Vote              & \cmark & \xmark & 70.2\% & 54.8\% \\
        Agent with Python Tools    & \cmark & \xmark & 72.6\% & 45.2\% \\
        \midrule
        \multicolumn{5}{l}{\textit{New Agents}} \\

        % -------- Solver--Verifier variants --------
        \textbf{Solver--Verifier (Base)} 
            &
            \begin{tabular}{@{}c@{}}
                Solver: \xmark \\[-2pt]
                Verifier: \xmark
            \end{tabular}
            & \xmark 
            & 83.4\% 
            & 66.2\% \\

        \textbf{Solver--Verifier (SFT Solver Only)} 
            &
            \begin{tabular}{@{}c@{}}
                Solver: \cmark \\[-2pt]
                Verifier: \xmark
            \end{tabular}
            & \xmark 
            & 86.0\% 
            & 67.0\% \\

        \textbf{Solver--Verifier (SFT Verifier Only)} 
            &
            \begin{tabular}{@{}c@{}}
                Solver: \xmark \\[-2pt]
                Verifier: \cmark
            \end{tabular}
            & \xmark 
            & 84.0\%$^\ast$
            & 67.4\%$^\ast$ \\

        \textbf{Solver--Verifier (SFT Both, No RL)} 
            &
            \begin{tabular}{@{}c@{}}
                Solver: \cmark \\[-2pt]
                Verifier: \cmark
            \end{tabular}
            & \xmark 
            & 86.4\%$^\ast$
            & 68.0\%$^\ast$ \\

        \textbf{Solver--Verifier (SFT Both + RL)} 
            &
            \begin{tabular}{@{}c@{}}
                Solver: \cmark \\[-2pt]
                Verifier: \cmark
            \end{tabular}
            & \cmark 
            & 86.8\%$^\ast$
            & 68.8\%$^\ast$ \\

        \addlinespace

        % -------- Code Feedback agent --------
        \textbf{Agent with Code Feedback (Base)}     
            & \xmark & \xmark & 76.4\%$^\ast$ & 60.0\%$^\ast$ \\
        \textbf{Agent with Code Feedback (SFT Only)} 
            & \cmark & \xmark & 82.8\%$^\ast$ & 66.0\%$^\ast$ \\
        \textbf{Agent with Code Feedback (SFT + RL)} 
            & \cmark & \cmark & 84.6\%$^\ast$ & 67.8\% \\
        
        \bottomrule
    \end{tabular}
\end{table}

 




\todoRed{Replace placeholder values (XX.X, YY.Y) with your final GRPO results and add additional rows for any further agents or model variants you test for the final report.}
\todoBlue{Ensure the caption and surrounding text succinctly explain the experimental setting: dataset sizes, evaluation metric, and which model checkpoints each configuration uses.}
\todoGreen{Explicitly note in the text that some agents hurt performance compared to the SFT or even base model, and briefly hint at the main causes, which you then analyze in more depth in the Analysis section.}

\subsection{Qualitative Analysis}
\label{subsec:qual}

In addition to numerical accuracy, we inspect model outputs qualitatively.

\todoRed{Include 2--4 representative examples: (1) a problem solved correctly only after SFT and/or RL; (2) a problem where a Solver--Checker agent fixes an incorrect single-model answer; (3) a problem where an agent turns a previously correct answer into an incorrect one; and optionally (4) a hard failure case that none of your methods solve.}
\todoBlue{Use these examples to describe general patterns in outputs (e.g., more structured reasoning, fewer arithmetic mistakes, or systematic misunderstandings) and tie these patterns back to your quantitative results.}
\todoGreen{Choose at least one example that clearly illustrates why some agents degrade performance (e.g., checker mis-evaluating steps, information loss due to summarization, or confusion from long multi-turn contexts) and highlight these failure modes explicitly.}

\section{Analysis}
\label{sec:analysis}

Our results demonstrate substantial improvements through both training and inference-time
interventions. Supervised fine-tuning yields rapid convergence, with both LoRA and full
fine-tuning producing comparable results on GSM8K and MATH-500. Cross-dataset transfer from
GSM8K to MATH-500 indicates general reasoning improvement rather than narrow memorization.

GRPO training is more computationally demanding---it requires generating multiple candidate
responses per prompt and computing reward signals---but is expected to provide further gains by
directly optimizing correctness under the evaluation metric.

Agentic workflows, particularly Solver--Checker with tools, Majority Vote, and the Python-tools
agent, outperform the base model by leveraging external computation and ensemble robustness.
Other tested workflows (e.g., summarizer variants, stateless checkers) exhibit performance
degradation (GSM8K-test 46.8\% to 59.4\%) due to:
\begin{itemize}
    \item insufficient model capacity for complex multi-agent coordination,
    \item limited context windows constraining elaborate prompts, and
    \item over-specialized role designs that reduce cross-task generalization.
\end{itemize}

\todoGreen{Add a focused paragraph analyzing why certain agents hurt performance. Use concrete evidence from the qualitative examples in Section~\ref{subsec:qual} and link those failure modes to specific design choices (e.g., noisy checker feedback, lossy summarization, or conflicting instructions).}
\todoRed{Explicitly discuss key limitations of your approach: reliance on strong teacher models, compute cost of RL and multi-agent inference, limited evaluation domains, and any instability observed during training or deployment.}
\todoBlue{Tighten the logical flow from experimental setup to conclusions, ensuring each claim is supported by either quantitative results or the qualitative trends you discussed.}

\section{Conclusion}
\label{sec:conclusion}

\todoRed{Write a concise conclusion (2--3 paragraphs) that: (1) restates the problem and your approach; (2) highlights your best-performing configuration and main quantitative improvements; (3) summarizes the main qualitative insights about how SFT, RL, and agents change reasoning behavior; and (4) clearly notes the main limitations.}
\todoBlue{Connect your findings back to the broader literature, emphasizing what insight your project adds about small LMs and math reasoning compared to prior work.}
\todoGreen{Briefly reflect on the relative contributions of RL and agentic design in your current system, and suggest which direction seems most promising for future iterations given your results and the TA's emphasis on these components.}

\section{Future Work}
\label{sec:future-work}

Our future directions include:
\begin{itemize}
    \item \textbf{Training optimization and data expansion:} optimizing training configurations and
    hyperparameters, and expanding to the full MATH dataset and competition problems
    such as AIME and AMC.
    \item \textbf{Advanced RL methods:} improving reward mechanisms through fine-grained rule design
    (formatting, step validity), reward models, and process supervision; exploring advanced
    sampling strategies integrating Monte Carlo Tree Search (MCTS) for
    more effective exploration.
    \item \textbf{Agentic RL (most important):} training models through agentic workflows combined with
    domain expert models, enabling joint optimization of multi-agent
    reasoning and policy learning to achieve synergistic improvements beyond independent
    training and inference-time augmentation.
\end{itemize}

\todoBlue{Tie each future direction to specific limitations or observations from your experiments (e.g., data scarcity on certain topics, RL instability, or brittle multi-agent coordination).}
\todoGreen{Highlight which future steps you see as most impactful for improving RL performance and agent robustness, acknowledging the points raised in the TA feedback.}

\section{Team Contributions}

\todoRed{Add 1--2 sentences per team member summarizing their main contributions (data pipeline, training, agent design, experiments, analysis, writing, etc.), as required by the final project instructions.}
\todoBlue{Ensure that the described contributions are consistent with responsibilities implied by the rest of the report.}

\section*{Acknowledgments (Optional)}

\todoRed{Optionally thank your TA mentor, classmates who gave feedback, and any providers of computational resources or tools that were particularly helpful.}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
