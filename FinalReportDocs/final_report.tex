\documentclass{article}

\usepackage[final]{neurips_2019}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{float}

\title{
  SLM-Math: Final Report (Milestone-Aligned) \\
  \vspace{1em}
  \small{\normalfont Columbia COMS4705 Project} \\
  \small{\normalfont \textbf{Keywords:} \textit{Math Reasoning, Tool Use, Multi-Agent, RL, Small LMs}}
}

\author{
  Team SLM-Math
}

\begin{document}
\maketitle

\begin{abstract}
We extend the milestone work on tool-augmented, multi-agent math reasoning. Using Qwen2.5-Math-1.5B (base and SFT), we evaluate four agents: \texttt{solver\_verifier}, \texttt{agent\_code\_as\_answer}, \texttt{agent\_with\_code\_feedback}, and \texttt{solver\_with\_interactive\_code}. We report the best observed accuracies (GSM8K 500) and summarize ongoing GRPO-style RL attempts, which have not yet surpassed the supervised baseline due to weak reward signal and resource limits.
\end{abstract}

\section{Key Information}
\begin{itemize}
    \item TA mentor: Melody Ma
    \item External collaborators: No
    \item Sharing project: No
\end{itemize}

\section{Approach}
\subsection{Baseline Model}
Qwen2.5-Math-1.5B (1.5B params), pretrained on math corpora; SFT checkpoint \texttt{sft\_full\_lr5e5\_ckpt298\_81.6acc} serves as our strongest starting point.

\subsection{Pipeline}
\begin{itemize}
    \item Data: GSM8K (+ some MATH); CoT-style prompts with \texttt{\textbackslash boxed\{\}} answers.
    \item SFT: full fine-tuning; LoRA variants exist (prior milestone), but current best is full SFT.
    \item RL (GRPO-style): LoRA on solver\_verifier; rewards for correctness, consistency; KL optional; batch/seq trimmed to avoid OOM.
    \item Agents: tool-augmented and verifier-based workflows (see below).
\end{itemize}

\section{Agents Considered}
\begin{itemize}
    \item \textbf{solver\_verifier} (\texttt{agent/solver\_verifier.py}): solver generates reasoning+code; code self-check; verifier arbitrates.
    \item \textbf{agent\_code\_as\_answer}: uses code execution result directly as the final answer when present.
    \item \textbf{agent\_with\_code\_feedback}: two-pass; executes code, then asks model to revise the \texttt{\textbackslash boxed\{\}} answer using the execution output.
    \item \textbf{solver\_with\_interactive\_code}: step-by-step code execution inside a single conversation turn with iterative feedback.
\end{itemize}

\section{Experiments}
\textbf{Datasets:} GSM8K (500 eval slice for current best numbers), MATH (for broader stress). \\
\textbf{Metrics:} Pass@1 / exact-match on final \texttt{\textbackslash boxed\{\}} answer.

\subsection*{Results (Best Observed)}

\begin{table}[H]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Agent / Config} & \textbf{GSM8K (500)} & \textbf{Notes} \\
\midrule
solver\_verifier (SFT ckpt) & \textbf{84.0\%} & run: \texttt{test\_solver\_verifier\_...0718}; first-try 81.4\% \\
solver\_verifier (SFT ckpt, earlier) & 83.4\% & run: \texttt{test\_solver\_verifier\_sft\_...1208} \\
base\_direct (SFT ckpt, plain) & 81.6\% & run: \texttt{my\_evaluation\_...1208}; plain prompt \\
agent\_with\_code\_feedback & 81.6\% & run: \texttt{test\_code\_feedback\_sft\_...1208}; first-try 79.2\% \\
agent\_default\_prompt\_with\_code & 77.4\% & run: \texttt{test\_default\_prompt\_code\_...1208} \\
solver\_verifier (base) & 65.8\% & from milestone table \\
\midrule
agent\_code\_as\_answer & (TBD) & not yet re-run post-SFT (milestone single-shot python tools: 72.6\%) \\
solver\_with\_interactive\_code & (TBD) & not yet re-run post-SFT \\
\midrule
SFT (Full) & 81.6\% & milestone baseline \\
SFT (LoRA r=16) & 80.0\% & milestone baseline \\
Solver-Checker With-Tools (milestone) & 81.4\% & milestone agentic workflow \\
\bottomrule
\end{tabular}
\caption{Best observed accuracies. Only \texttt{solver\_verifier} has updated GSM8K results post-SFT (84.0\%). Other agents need refreshed runs; milestone numbers shown where available.}
\label{tab:results}
\end{table}

\subsection*{RL Status (GRPO-style, LoRA)}
\begin{itemize}
    \item Runs: \texttt{results/rl\_solver\_verifier\_20251208\_*}, \texttt{20251209\_012706}, \texttt{20251209\_013726}, \texttt{20251209\_043506} (partial).
    \item Training accuracy: stabilizes around $0.62$--$0.68$ (batch-level); raw\_reward now logged (whitening previously masked signal).
    \item OOM fixes: smaller batch (5, often 4), grad\_acc=6, max length 1792, KL=0 (skip ref model); PYTORCH\_CUDA\_ALLOC\_CONF tuned.
    \item No held-out RL eval yet (to avoid train/eval leakage); must run post-RL GSM8K test eval on saved checkpoints once stable.
\end{itemize}

\section{Analysis}
\begin{itemize}
    \item Supervised + agentic: \texttt{solver\_verifier} + SFT + code exec + verifier yields 84.0\% on GSM8K (500).
    \item RL so far: no gains over SFT due to weak/whitened rewards and resource limits; exploration limited (num\_return\_sequences=1).
    \item Data mixing (gsm8k + math) depresses training-batch accuracy relative to gsm8k-only eval.
\end{itemize}

\section{Limitations and Next Steps}
\begin{itemize}
    \item Evaluate RL checkpoints on GSM8K test to measure real gains.
    \item Reward shaping: disable whitening or add EMA baseline; boost consistency reward; try num\_return\_sequences $>$ 1.
    \item Resource: if OOM persists, batch 5$\to$4 or 3; max length 1792$\to$1536.
    \item Re-run agents: refresh GSM8K results for \texttt{agent\_code\_as\_answer}, \texttt{agent\_with\_code\_feedback}, \texttt{solver\_with\_interactive\_code}.
    \item Add held-out dev split for light in-training eval without leakage.
\end{itemize}

\section{Response to TA Feedback}
\begin{itemize}
    \item Clearer eval protocol: avoid train/test leakage; run post-RL eval on GSM8K test.
    \item Analyze failures: in the 84\% run, 72/500 failures remain; improved 14, degraded 1 â€” need error taxonomy.
    \item Report reproducibility: commands (\texttt{scripts/eval\_solver\_verifier.sh}, \texttt{scripts/train\_rl\_solver\_verifier\_current\_v2.sh}), hyperparams (batch, grad\_acc, max\_len, KL).
\end{itemize}

\end{document}

