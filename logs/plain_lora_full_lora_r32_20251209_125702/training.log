2025-12-09 12:57:02,458 - INFO - ================================================================================
2025-12-09 12:57:02,458 - INFO - Training Configuration
2025-12-09 12:57:02,458 - INFO - ================================================================================
2025-12-09 12:57:02,458 - INFO - Round Name: plain_lora_full
2025-12-09 12:57:02,458 - INFO - Mode: lora
2025-12-09 12:57:02,458 - INFO - Model: pretrained_models/Qwen2.5-Math-1.5B
2025-12-09 12:57:02,458 - INFO - Data: data/reasoning_code_v2/run_1209_1011/reasoning_code_v2_gsm8k_train_math_train_1209_1011_code_box_eq_gt.jsonl
2025-12-09 12:57:02,458 - INFO - Output: checkpoints/plain_lora_full_lora_r32_20251209_125702
2025-12-09 12:57:02,458 - INFO - Logs: logs/plain_lora_full_lora_r32_20251209_125702
2025-12-09 12:57:02,458 - INFO - GPUs: 0
2025-12-09 12:57:02,458 - INFO - Epochs: 4
2025-12-09 12:57:02,458 - INFO - Batch size: 20
2025-12-09 12:57:02,458 - INFO - Gradient accumulation: 8
2025-12-09 12:57:02,458 - INFO - Max sequence length: 2048
2025-12-09 12:57:02,458 - INFO - Gradient checkpointing: True
2025-12-09 12:57:02,458 - INFO - Enable evaluation: True
2025-12-09 12:57:02,458 - INFO - Evaluation frequency: every 1 epoch(s)
2025-12-09 12:57:02,458 - INFO - LoRA rank: 32
2025-12-09 12:57:02,458 - INFO - ================================================================================
2025-12-09 12:57:02,458 - INFO - Loading data from: data/reasoning_code_v2/run_1209_1011/reasoning_code_v2_gsm8k_train_math_train_1209_1011_code_box_eq_gt.jsonl
2025-12-09 12:57:02,885 - INFO - Filtered to 13959 correct samples
2025-12-09 12:57:02,885 - INFO - Total samples: 13959
2025-12-09 12:57:02,885 - INFO - Sample columns: ['index', 'question', 'ground_truth', 'predicted_answer', 'code_answer', 'boxed_answer', 'answers_match', 'thinking_process', 'solution', 'raw_output', 'code', 'execution', 'answer_source', 'correct', 'teacher_model', 'thinking_effort', 'round', 'attempt_count', 'attempts_history', 'tokens_used', 'status', 'dataset']
2025-12-09 12:57:02,885 - INFO - Sample question: Lisa, Jack, and Tommy earned $60 from washing cars all week. However, half of the $60 was earned by ...
2025-12-09 12:57:03,192 - INFO - Training dataset size: 13959
2025-12-09 12:57:03,222 - INFO - Loading model: pretrained_models/Qwen2.5-Math-1.5B
2025-12-09 12:57:04,600 - INFO - Model loaded on: cuda:0
2025-12-09 12:57:04,600 - INFO - Configuring LoRA with rank=32
2025-12-09 12:57:04,601 - INFO - Dataset size: 13959
2025-12-09 12:57:04,601 - INFO - Steps per epoch: 88
2025-12-09 12:57:04,601 - INFO - Save checkpoint every 87912 steps (999 epochs)
2025-12-09 12:57:04,601 - INFO - 
Loading evaluation datasets...
2025-12-09 12:57:04,601 - INFO - Evaluation will run every 1 epoch(s)
2025-12-09 12:57:04,607 - INFO - Loaded 20 samples from gsm8k
2025-12-09 12:57:04,610 - INFO - Loaded 20 samples from math500
2025-12-09 12:57:04,610 - INFO - Loaded evaluation datasets: ['gsm8k', 'math500']
2025-12-09 12:57:04,610 - INFO - Training metrics will be saved to: logs/plain_lora_full_lora_r32_20251209_125702/training_metrics.csv
2025-12-09 12:57:04,610 - INFO - Using learning rate: 0.0001
