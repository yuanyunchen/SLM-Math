2025-11-25 11:31:32,097 - INFO - ================================================================================
2025-11-25 11:31:32,097 - INFO - Training Configuration
2025-11-25 11:31:32,097 - INFO - ================================================================================
2025-11-25 11:31:32,097 - INFO - Round Name: exp_lora_sft
2025-11-25 11:31:32,097 - INFO - Mode: lora
2025-11-25 11:31:32,097 - INFO - Model: pretrained_models/Qwen2.5-Math-1.5B
2025-11-25 11:31:32,098 - INFO - Data: data/cot_generated/first_round_final_gsm8k_math/first_round_final_gsm8k_math.json
2025-11-25 11:31:32,098 - INFO - Output: checkpoints/exp_lora_sft_lora_r16_20251125_113132
2025-11-25 11:31:32,098 - INFO - Logs: logs/exp_lora_sft_lora_r16_20251125_113132
2025-11-25 11:31:32,098 - INFO - GPUs: 2
2025-11-25 11:31:32,098 - INFO - Epochs: 5
2025-11-25 11:31:32,098 - INFO - Batch size: 16
2025-11-25 11:31:32,098 - INFO - Gradient accumulation: 4
2025-11-25 11:31:32,098 - INFO - Max sequence length: 2048
2025-11-25 11:31:32,098 - INFO - Gradient checkpointing: True
2025-11-25 11:31:32,098 - INFO - Enable evaluation: True
2025-11-25 11:31:32,098 - INFO - Evaluation frequency: every 1 epoch(s)
2025-11-25 11:31:32,098 - INFO - LoRA rank: 16
2025-11-25 11:31:32,098 - INFO - ================================================================================
2025-11-25 11:31:32,098 - INFO - Loading data from: data/cot_generated/first_round_final_gsm8k_math/first_round_final_gsm8k_math.json
2025-11-25 11:31:32,909 - INFO - Filtered to 18946 correct samples
2025-11-25 11:31:32,909 - INFO - Total samples: 18946
2025-11-25 11:31:32,909 - INFO - Sample columns: ['index', 'question', 'ground_truth', 'predicted_answer', 'thinking_process', 'solution', 'raw_output', 'teacher_model', 'tokens_used', 'finish_reason', 'correct', 'status', '_validation', 'generation_round', 'dataset', 'attempt_count', 'attempts_history']
2025-11-25 11:31:32,909 - INFO - Sample question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How m...
2025-11-25 11:31:33,602 - INFO - Training dataset size: 18946
2025-11-25 11:31:33,628 - INFO - Loading model: pretrained_models/Qwen2.5-Math-1.5B
2025-11-25 11:31:35,175 - INFO - Model loaded on: cuda:0
2025-11-25 11:31:35,175 - INFO - Configuring LoRA with rank=16
2025-11-25 11:31:35,175 - INFO - Dataset size: 18946
2025-11-25 11:31:35,175 - INFO - Steps per epoch: 297
2025-11-25 11:31:35,175 - INFO - Save checkpoint every 296703 steps (999 epochs)
2025-11-25 11:31:35,175 - INFO - 
Loading evaluation datasets...
2025-11-25 11:31:35,175 - INFO - Evaluation will run every 1 epoch(s)
2025-11-25 11:31:35,185 - INFO - Loaded 200 samples from gsm8k
2025-11-25 11:31:35,189 - INFO - Loaded 200 samples from math500
2025-11-25 11:31:35,189 - INFO - Loaded evaluation datasets: ['gsm8k', 'math500']
2025-11-25 11:31:35,189 - INFO - Training metrics will be saved to: logs/exp_lora_sft_lora_r16_20251125_113132/training_metrics.csv
2025-11-25 11:31:35,189 - INFO - Using learning rate: 0.0001
2025-11-25 11:32:23,486 - INFO - Starting training...
2025-11-25 12:38:54,892 - INFO - Epoch 1.0 completed at step 297
2025-11-25 12:38:54,893 - INFO - 
================================================================================
2025-11-25 12:38:54,893 - INFO - Running evaluation at epoch 1
2025-11-25 12:38:54,893 - INFO - ================================================================================
2025-11-25 12:38:54,893 - INFO - Evaluating on gsm8k...
2025-11-25 13:57:41,242 - INFO - Saved 10 sample logs to logs/exp_lora_sft_lora_r16_20251125_113132/eval_samples_gsm8k_20251125_135741.txt
2025-11-25 13:57:41,242 - INFO -   gsm8k accuracy: 0.8400
2025-11-25 13:57:41,242 - INFO - Evaluating on math500...
2025-11-25 15:47:47,644 - INFO - Saved 10 sample logs to logs/exp_lora_sft_lora_r16_20251125_113132/eval_samples_math500_20251125_154747.txt
2025-11-25 15:47:47,645 - INFO -   math500 accuracy: 0.6550
2025-11-25 15:47:47,648 - INFO - Logged evaluation results to W&B
2025-11-25 15:47:47,648 - INFO - ================================================================================

