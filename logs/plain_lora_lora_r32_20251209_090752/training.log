2025-12-09 09:07:52,619 - INFO - ================================================================================
2025-12-09 09:07:52,619 - INFO - Training Configuration
2025-12-09 09:07:52,619 - INFO - ================================================================================
2025-12-09 09:07:52,619 - INFO - Round Name: plain_lora
2025-12-09 09:07:52,619 - INFO - Mode: lora
2025-12-09 09:07:52,619 - INFO - Model: pretrained_models/Qwen2.5-Math-1.5B
2025-12-09 09:07:52,619 - INFO - Data: data/reasoning_code/run_1208_1337/reasoning_code_gsm8k_train_math_train_1208_1337_code_boxed_correct.json
2025-12-09 09:07:52,619 - INFO - Output: checkpoints/plain_lora_lora_r32_20251209_090752
2025-12-09 09:07:52,619 - INFO - Logs: logs/plain_lora_lora_r32_20251209_090752
2025-12-09 09:07:52,619 - INFO - GPUs: 2
2025-12-09 09:07:52,619 - INFO - Epochs: 4
2025-12-09 09:07:52,619 - INFO - Batch size: 20
2025-12-09 09:07:52,619 - INFO - Gradient accumulation: 8
2025-12-09 09:07:52,619 - INFO - Max sequence length: 2048
2025-12-09 09:07:52,619 - INFO - Gradient checkpointing: True
2025-12-09 09:07:52,619 - INFO - Enable evaluation: True
2025-12-09 09:07:52,619 - INFO - Evaluation frequency: every 1 epoch(s)
2025-12-09 09:07:52,619 - INFO - LoRA rank: 32
2025-12-09 09:07:52,619 - INFO - ================================================================================
2025-12-09 09:07:52,619 - INFO - Loading data from: data/reasoning_code/run_1208_1337/reasoning_code_gsm8k_train_math_train_1208_1337_code_boxed_correct.json
2025-12-09 09:07:52,951 - INFO - Filtered to 15773 correct samples
2025-12-09 09:07:52,951 - INFO - Total samples: 15773
2025-12-09 09:07:52,951 - INFO - Sample columns: ['index', 'question', 'ground_truth', 'predicted_answer', 'code_answer', 'boxed_answer', 'answers_match', 'thinking_process', 'solution', 'raw_output', 'code', 'execution', 'answer_source', 'correct', 'teacher_model', 'thinking_effort', 'round', 'attempt_count', 'attempts_history', 'tokens_used', 'status', 'dataset']
2025-12-09 09:07:52,951 - INFO - Sample question: Tim has 30 less apples than Martha, and Harry has half as many apples as Tim. If Martha has 68 apple...
2025-12-09 09:07:53,141 - INFO - Training dataset size: 15773
2025-12-09 09:07:53,162 - INFO - Loading model: pretrained_models/Qwen2.5-Math-1.5B
2025-12-09 09:07:54,556 - INFO - Model loaded on: cuda:0
2025-12-09 09:07:54,557 - INFO - Configuring LoRA with rank=32
2025-12-09 09:07:54,557 - INFO - Dataset size: 15773
2025-12-09 09:07:54,557 - INFO - Steps per epoch: 99
2025-12-09 09:07:54,557 - INFO - Save checkpoint every 98901 steps (999 epochs)
2025-12-09 09:07:54,557 - INFO - 
Loading evaluation datasets...
2025-12-09 09:07:54,557 - INFO - Evaluation will run every 1 epoch(s)
2025-12-09 09:07:54,565 - INFO - Loaded 20 samples from gsm8k
2025-12-09 09:07:54,568 - INFO - Loaded 20 samples from math500
2025-12-09 09:07:54,568 - INFO - Loaded evaluation datasets: ['gsm8k', 'math500']
2025-12-09 09:07:54,568 - INFO - Training metrics will be saved to: logs/plain_lora_lora_r32_20251209_090752/training_metrics.csv
2025-12-09 09:07:54,568 - INFO - Using learning rate: 0.0001
2025-12-09 09:08:16,041 - INFO - Starting training...
2025-12-09 09:25:33,003 - INFO - Epoch 1.0 completed at step 99
2025-12-09 09:25:33,004 - INFO - 
================================================================================
2025-12-09 09:25:33,004 - INFO - Running evaluation at epoch 1
2025-12-09 09:25:33,004 - INFO - ================================================================================
2025-12-09 09:25:33,004 - INFO - ================================================================================

2025-12-09 09:42:36,335 - INFO - Epoch 2.0 completed at step 198
2025-12-09 09:42:36,336 - INFO - 
================================================================================
2025-12-09 09:42:36,336 - INFO - Running evaluation at epoch 2
2025-12-09 09:42:36,336 - INFO - ================================================================================
2025-12-09 09:42:36,336 - INFO - ================================================================================

2025-12-09 09:59:44,198 - INFO - Epoch 3.0 completed at step 297
2025-12-09 09:59:44,198 - INFO - 
================================================================================
2025-12-09 09:59:44,198 - INFO - Running evaluation at epoch 3
2025-12-09 09:59:44,198 - INFO - ================================================================================
2025-12-09 09:59:44,199 - INFO - ================================================================================

2025-12-09 10:17:20,808 - INFO - Epoch 4.0 completed at step 396
2025-12-09 10:17:20,809 - INFO - 
================================================================================
2025-12-09 10:17:20,809 - INFO - Running evaluation at epoch 4
2025-12-09 10:17:20,809 - INFO - ================================================================================
2025-12-09 10:17:20,809 - INFO - ================================================================================

2025-12-09 10:17:20,814 - INFO - Training metrics saved to: logs/plain_lora_lora_r32_20251209_090752/training_metrics.csv
2025-12-09 10:17:20,816 - INFO - Metrics summary saved to: logs/plain_lora_lora_r32_20251209_090752/metrics_summary.txt
2025-12-09 10:17:20,817 - INFO - Saving final model to: checkpoints/plain_lora_lora_r32_20251209_090752/final_model
2025-12-09 10:17:20,995 - INFO - Training completed!
2025-12-09 10:17:20,995 - INFO - Final model saved to: checkpoints/plain_lora_lora_r32_20251209_090752/final_model
2025-12-09 10:17:20,995 - INFO - Training metrics: {'train_runtime': 4144.5385, 'train_samples_per_second': 15.223, 'train_steps_per_second': 0.096, 'total_flos': 3.169676426939105e+17, 'train_loss': 0.45438917357512193, 'entropy': 0.36426327061653135, 'num_tokens': 18421684.0, 'mean_token_accuracy': 0.9007101607322693, 'epoch': 4.0}
