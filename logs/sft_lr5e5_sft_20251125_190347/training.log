2025-11-25 19:03:47,913 - INFO - ================================================================================
2025-11-25 19:03:47,913 - INFO - Training Configuration
2025-11-25 19:03:47,913 - INFO - ================================================================================
2025-11-25 19:03:47,913 - INFO - Round Name: sft_lr5e5
2025-11-25 19:03:47,913 - INFO - Mode: sft
2025-11-25 19:03:47,913 - INFO - Model: pretrained_models/Qwen2.5-Math-1.5B
2025-11-25 19:03:47,913 - INFO - Data: data/cot_generated/first_round_final_gsm8k_math/first_round_final_gsm8k_math.json
2025-11-25 19:03:47,913 - INFO - Output: checkpoints/sft_lr5e5_sft_20251125_190347
2025-11-25 19:03:47,913 - INFO - Logs: logs/sft_lr5e5_sft_20251125_190347
2025-11-25 19:03:47,913 - INFO - GPUs: 0
2025-11-25 19:03:47,913 - INFO - Epochs: 2
2025-11-25 19:03:47,913 - INFO - Batch size: 16
2025-11-25 19:03:47,913 - INFO - Gradient accumulation: 8
2025-11-25 19:03:47,913 - INFO - Max sequence length: 2048
2025-11-25 19:03:47,913 - INFO - Gradient checkpointing: True
2025-11-25 19:03:47,913 - INFO - Enable evaluation: True
2025-11-25 19:03:47,913 - INFO - Evaluation frequency: every 999 epoch(s)
2025-11-25 19:03:47,913 - INFO - ================================================================================
2025-11-25 19:03:47,913 - INFO - Loading data from: data/cot_generated/first_round_final_gsm8k_math/first_round_final_gsm8k_math.json
2025-11-25 19:03:48,667 - INFO - Filtered to 18946 correct samples
2025-11-25 19:03:48,668 - INFO - Total samples: 18946
2025-11-25 19:03:48,668 - INFO - Sample columns: ['index', 'question', 'ground_truth', 'predicted_answer', 'thinking_process', 'solution', 'raw_output', 'teacher_model', 'tokens_used', 'finish_reason', 'correct', 'status', '_validation', 'generation_round', 'dataset', 'attempt_count', 'attempts_history']
2025-11-25 19:03:48,668 - INFO - Sample question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How m...
2025-11-25 19:03:49,350 - INFO - Training dataset size: 18946
2025-11-25 19:03:49,463 - INFO - Loading model: pretrained_models/Qwen2.5-Math-1.5B
2025-11-25 19:03:50,945 - INFO - Model loaded on: cuda:0
2025-11-25 19:03:50,945 - INFO - Dataset size: 18946
2025-11-25 19:03:50,945 - INFO - Steps per epoch: 149
2025-11-25 19:03:50,945 - INFO - Save checkpoint every 148851 steps (999 epochs)
2025-11-25 19:03:50,945 - INFO - 
Loading evaluation datasets...
2025-11-25 19:03:50,945 - INFO - Evaluation will run every 999 epoch(s)
2025-11-25 19:03:50,952 - INFO - Loaded 500 samples from gsm8k
2025-11-25 19:03:50,955 - INFO - Loaded 500 samples from math500
2025-11-25 19:03:50,955 - INFO - Loaded evaluation datasets: ['gsm8k', 'math500']
2025-11-25 19:03:50,955 - INFO - Training metrics will be saved to: logs/sft_lr5e5_sft_20251125_190347/training_metrics.csv
2025-11-25 19:03:50,955 - INFO - Using learning rate: 5e-05
2025-11-25 19:04:36,528 - INFO - Starting training...
2025-11-25 20:22:29,924 - INFO - Epoch 1.0 completed at step 149
2025-11-25 21:40:20,755 - INFO - Epoch 2.0 completed at step 298
2025-11-25 21:40:20,760 - INFO - Training metrics saved to: logs/sft_lr5e5_sft_20251125_190347/training_metrics.csv
2025-11-25 21:40:20,761 - INFO - Metrics summary saved to: logs/sft_lr5e5_sft_20251125_190347/metrics_summary.txt
2025-11-25 21:40:20,762 - INFO - Saving final model to: checkpoints/sft_lr5e5_sft_20251125_190347/final_model
2025-11-25 21:40:24,576 - INFO - Training completed!
2025-11-25 21:40:24,576 - INFO - Final model saved to: checkpoints/sft_lr5e5_sft_20251125_190347/final_model
2025-11-25 21:40:24,576 - INFO - Training metrics: {'train_runtime': 9344.0699, 'train_samples_per_second': 4.055, 'train_steps_per_second': 0.032, 'total_flos': 5.301889895019233e+17, 'train_loss': 0.5278008196177899, 'entropy': 0.4955395699417504, 'num_tokens': 37121764.0, 'mean_token_accuracy': 0.8624272511823334, 'epoch': 2.0}
